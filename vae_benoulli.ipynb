{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img style=\"float:left; border-radius:50%\" src=\"https://avatars2.githubusercontent.com/u/58553327?s=460&u=3276252f07fb379c248bc8c9ce344bfdcaed7c45&v=4\" width=\"40px\">\n",
    "<a href=\"https://github.com/carlosholivan\"><img src=\"https://www.sferalabs.cc/wp-content/uploads/github-logo.png\" width=70px style=\"float: right;\"></a>\n",
    "</div>\n",
    "\n",
    "<a name=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. BERNOULLI VAE <a name=\"top\"></a>\n",
    "\n",
    "In this notebook it is described how to compute different time-frequency inputs with <strong>compute_input_from_audiopath</strong> function.\n",
    "\n",
    "Author: Carlos Hernández Oliván<br>\n",
    "Last update: 7 November 2020\n",
    "  \n",
    "\n",
    "### Table of Contents <a name=\"index\"></a>\n",
    " \n",
    "1. [Short-Time Fourier Transform](#stft)\n",
    "2. [Chroma from STFT](#chroma-stft)\n",
    "3. [Constant Q-Transform](#cqt)\n",
    "    \n",
    "[References](#references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# module for reading audiofile\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#torch modules\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "class Config:\n",
    "    pass\n",
    "\n",
    "class InputsConfig(Config):\n",
    "    \n",
    "    SAMPLING_RATE = 16000  #  Hz\n",
    "    \n",
    "    WINDOW_SIZE_MS = 40  # ms\n",
    "    WINDOW_SIZE = int(WINDOW_SIZE_MS / 1000 * SAMPLING_RATE)\n",
    "    \n",
    "    #HOP_LENGTH_MS = 10  #  ms\n",
    "    #HOP_LENGTH = int(HOP_LENGTH_MS / 1000 * SAMPLING_RATE)\n",
    "    HOP_LENGTH = 512\n",
    "    \n",
    "    F_MIN = 32.70  #  minimum frequency in Hz\n",
    "    BINS = 190\n",
    "    BINS_PER_OCTAVE = 24\n",
    "    \n",
    "    \n",
    "class ParamsConfig(Config):\n",
    "    \n",
    "    DATA_PATH = './data'\n",
    "    TRAINED_MODELS_PATH = './trained_models'\n",
    "    \n",
    "    BATCH_SIZE  =  1\n",
    "    NUM_CHANNELS = 2  # output channels after first convolution\n",
    "    LEARNING_RATE = 1e-3\n",
    "    NUM_EPOCHS = 100\n",
    "    \n",
    "    VAE_BETA = 1\n",
    "    LATENT_DIMS = 2\n",
    "    \n",
    "    LOG_INTERVAL = 10\n",
    "    SEED = 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-735860145ef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mParamsConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "ParamsConfig(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing data\n",
    "\n",
    "In this part we'll create and store as .npy the audio representation we'd want to use as the input of our model.\n",
    "The .npy files will be stored in data/ directory in our module and the name of the files will be the same as the audio files names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mp3_to_wav(audio_file_path, delete_mp3=False):\n",
    "    \"\"\"This function converts an mp3 file to a wav file\"\"\"\n",
    "    \n",
    "    if audio_file_path.split(\".\",1)[1] == 'mp3':\n",
    "        try:\n",
    "            sound = AudioSegment.from_mp3(audio_file_path)\n",
    "            audio_wav_file_path = audio_file_path.split(\".\",1)[0] + \".wav\"\n",
    "            new_sound = sound.export(audio_wav_file_path, format=\"wav\") #convert to wav file\n",
    "            print(audio_file_path, 'file converted from', audio_file_path.split(\".\",1)[1], 'to wav format')\n",
    "            \n",
    "            #delete mp3 file\n",
    "            if delete_mp3:\n",
    "                os.remove(audio_file_path)\n",
    "                \n",
    "            return audio_wav_file_path\n",
    "                \n",
    "        except:\n",
    "            raise ValueError('File cannot be converted to wav')\n",
    "    else:\n",
    "        raise ValueError('Inserted file is not an mp3 file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_input(audio_file_path):\n",
    "    \"\"\"This function computes the centroid of an audio file given its path\"\"\"\n",
    "    \n",
    "    try:\n",
    "        y, sr = librosa.load(audio_file_path, sr=None)  \n",
    "        \n",
    "    except:\n",
    "        #Call convert_mp3_to_wav to convert mp3 to wav\n",
    "        new_wav_path = convert_mp3_to_wav(audio_file_path)\n",
    "        y, sr = librosa.load(new_wav_path, sr=None)   \n",
    "        \n",
    "    #centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    cqt = np.abs(librosa.cqt(y, \n",
    "                             hop_length=InputsConfig.HOP_LENGTH, \n",
    "                             fmin=InputsConfig.F_MIN, \n",
    "                             n_bins=InputsConfig.BINS, \n",
    "                             bins_per_octave=InputsConfig.BINS_PER_OCTAVE))\n",
    "        \n",
    "    return cqt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_inputs(dataset_path):\n",
    "    \n",
    "    \"\"\"This function computes the centroids of all the audio files in dataset_path and\n",
    "    stores them in data directory in our module.\"\"\"\n",
    "    \n",
    "    data_path = './data'\n",
    "    \n",
    "    # Create data directory to store the input arrays\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    \n",
    "    for (im_dirpath, im_dirnames, im_filenames) in os.walk(dataset_path): \n",
    "        for f in im_filenames:  # loop in files\n",
    "            file_name = f.split(\".\",1)[0]\n",
    "            new_path = os.path.join(data_path, file_name + '.npy')\n",
    "            \n",
    "            if os.path.exists(new_path):  # if file is already in data, skip it\n",
    "                print(new_path, 'already exists')\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    audio_file_path = os.path.join(im_dirpath, f)  # get the audio file path\n",
    "                    inputs = compute_input(audio_file_path)  # compute the centroid of the audio file\n",
    "                    np.save(new_path, inputs)  # stores arrays in data directory\n",
    "\n",
    "                except:\n",
    "                    print('Skipping file:', f)\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compute the centroids and save them in our data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file: banjo.zip\n",
      "Skipping file: bass clarinet.zip\n",
      "Skipping file: bassoon.zip\n",
      "Skipping file: cello.zip\n",
      "Skipping file: clarinet.zip\n",
      "Skipping file: contrabassoon.zip\n",
      "Skipping file: cor anglais.zip\n",
      "Skipping file: flute.zip\n",
      "Skipping file: french horn.zip\n",
      "Skipping file: guitar.zip\n",
      "Skipping file: mandolin.zip\n",
      "Skipping file: oboe.zip\n",
      "Skipping file: saxophone.zip\n",
      "Skipping file: saxophone_Fs3_15_fortissimo_normal.mp3\n",
      "Skipping file: trombone.zip\n",
      "Skipping file: trumpet.zip\n",
      "Skipping file: tuba.zip\n",
      "Skipping file: viola.zip\n",
      "Skipping file: viola_D6_05_piano_arco-normal.mp3\n",
      "Skipping file: violin.zip\n",
      "Skipping file: double bass.zip\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/media/carlos/FILES/INVESTIGACION/Datasets/London Philarmonic Orchestra/'\n",
    "store_inputs(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Dataloader from London Philarmonic Orchestra Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args\n",
    "        ----\n",
    "            data_path : Path to all the array files\n",
    "            audio_file_path : Path of a single audio file\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.input_data = []\n",
    "        self.files_path = []\n",
    "        \n",
    "        for (im_dirpath, im_dirnames, im_filenames) in os.walk(self.data_path): \n",
    "            for f in im_filenames:  # loop in files\n",
    "                if f.split(\".\",1)[1] == 'npy':\n",
    "                    input_file_path = os.path.join(im_dirpath, f)  # get the audio file path\n",
    "                    input_file_data = np.load(input_file_path) # load npy file\n",
    "                    \n",
    "                    # append variables to lists\n",
    "                    self.input_data.append(input_file_data)\n",
    "                    self.files_path.append(input_file_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"count audio files\"\"\"\n",
    "        return len(self.input_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"take audio file form list\"\"\"\n",
    "                \n",
    "        input_data = self.input_data[index]\n",
    "        input_data = input_data[np.newaxis, :, :]  # add axis for batch\n",
    "        \n",
    "        audio_file = self.files_path[index]\n",
    "        \n",
    "        data = {\n",
    "                'file': os.path.split(audio_file)[1].split('.')[0], \n",
    "                'input': input_data\n",
    "                }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing Dataloader\n",
    "train_dataset = AudioDataset(data_path=ParamsConfig.DATA_PATH)\n",
    "train_trainloader = DataLoader(train_dataset, batch_size = ParamsConfig.BATCH_SIZE, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the training dataset: 13681\n",
      "<class 'dict'>\n",
      "['cello_C6_15_fortissimo_arco-normal']\n",
      "torch.Size([1, 1, 190, 167])\n"
     ]
    }
   ],
   "source": [
    "print('Number of files in the training dataset:', len(train_dataset))\n",
    "\n",
    "# Get a sample of the dataset\n",
    "dataiter = iter(train_trainloader)\n",
    "data = dataiter.next()\n",
    "print(type(data))\n",
    "print(data['file'])\n",
    "print(data['input'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13681"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot one sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(train_dataset.input_data[700], cmap='viridis', interpolation='none', aspect=20, origin='lower')\n",
    "ax.set_title(train_dataset.files_path[700])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 31730])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input'].view(data['input'].size(0), -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_channels=ParamsConfig.NUM_CHANNELS, latent_dims=ParamsConfig.LATENT_DIMS):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        self.latent_dims = latent_dims\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, \n",
    "                               out_channels = self.num_channels*32, \n",
    "                               kernel_size = (5,5), \n",
    "                               stride = (1,1), \n",
    "                               padding = ((5 - 1) // 2, (5 - 1) // 2))\n",
    "        \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(5,3), stride=(5,1), padding=(1,1))\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = self.num_channels, \n",
    "                               out_channels = self.num_channels*64, \n",
    "                               kernel_size = (3,3), \n",
    "                               stride = (1,1), \n",
    "                               padding = ((3 - 1) // 2, (3 - 1)*3 // 2),\n",
    "                               dilation = (1,3))\n",
    "        \n",
    "        self.fc_mu = nn.Conv1d(in_channels = self.num_channels*64, \n",
    "                               out_channels = self.latent_dims,\n",
    "                               kernel_size = 1)\n",
    "        \n",
    "        self.fc_logvar = nn.Conv1d(in_channels = self.num_channels*64, \n",
    "                                   out_channels = self.latent_dims,\n",
    "                                   kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('Input size to Conv1 encoder:', x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        print('Output size Conv1 encoder', x.shape)\n",
    "        x = self.pool1(x)\n",
    "        print('Output size after mx-pool1', x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        print('Output size Conv2 encoder', x.shape)\n",
    "        x = x.view(1, -1, x.size(3)) # flatten batch of multi-channel feature maps\n",
    "        print('Output size after flatten', x.shape)\n",
    "        \n",
    "        x_mu = self.fc_mu(x)\n",
    "        print('Output size of mu after fc', x_mu.shape)\n",
    "        x_logvar = self.fc_logvar(x)\n",
    "        print('Output size of logvar after fc', x_logvar.shape)\n",
    "        return x_mu, x_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(64*2*7*7, 2)\n",
    "input = torch.randn(128, 64*2*7*7)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_channels=ParamsConfig.NUM_CHANNELS):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        self.fc = nn.Linear(in_features = ParamsConfig.LATENT_DIMS, \n",
    "                            out_features = self.num_channels*2*7*7)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels = self.num_channels*2, \n",
    "                                        out_channels = self.num_channels, \n",
    "                                        kernel_size = (4, 4), \n",
    "                                        stride = 2, \n",
    "                                        padding = 1)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels = self.num_channels, \n",
    "                                        out_channels = 1, \n",
    "                                        kernel_size = (4, 4), \n",
    "                                        stride = 2, \n",
    "                                        padding = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), self.num_channels*2, 7, 7) # unflatten batch of feature vectors to a batch of multi-channel feature maps\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv1(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder(x)\n",
    "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
    "        x_recon = self.decoder(latent)\n",
    "        return x_recon, latent_mu, latent_logvar\n",
    "    \n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            # the reparameterization trick\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "\n",
    "class Unflatten(nn.Module):\n",
    "    def __init__(self, channel, height, width):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), self.channel, self.height, self.width)\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_size):\n",
    "        super(ConvVAE, self).__init__()\n",
    "\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(6272, 1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # hidden => mu\n",
    "        self.fc1 = nn.Linear(1024, self.latent_size)\n",
    "\n",
    "        # hidden => logvar\n",
    "        self.fc2 = nn.Linear(1024, self.latent_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 6272),\n",
    "            nn.ReLU(),\n",
    "            Unflatten(128, 7, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "channels = 1\n",
    "h = train_dataset.input_data[700].shape[0]\n",
    "w = train_dataset.input_data[700].shape[1]\n",
    "\n",
    "summary(VAE(), input_size=(channels, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    \n",
    "<button type=\"button\" class=\"btn btn-primary\" style=\"float:left; background-color:#a273f9; border:0\"><a href=\"#top\" style=\"color:white; text-decoration: none\">⇦ Back to Top</a></button>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss<a name=\"loss\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(p_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(p_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. TRAIN FUNCTION<a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataset)/dataloader.batch_size)):\n",
    "        data = data['input']\n",
    "        data = data.to(device)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        bce_loss = criterion(reconstruction, data)\n",
    "        total_loss = loss(bce_loss, mu, logvar)\n",
    "        running_loss += total_loss.item()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    train_loss = running_loss/len(dataloader.dataset)\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. VALIDATION FUNCTION<a name=\"val\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(dataloader), total=int(len(val_dataset)/dataloader.batch_size)):\n",
    "            data = data['input']\n",
    "            data = data.to(device)\n",
    "            reconstruction, mu, logvar = model(data)\n",
    "            bce_loss = criterion(reconstruction, data)\n",
    "            total_loss = loss(bce_loss, mu, logvar)\n",
    "            running_loss += total_loss.item()\n",
    "  \n",
    "            # save the last batch input and output of every epoch\n",
    "            if i == int(len(val_dataset)/dataloader.batch_size) - 1:\n",
    "                num_rows = 8\n",
    "                both = torch.cat((data.view(ParamsConfig.BATCH_SIZE, 1, 28, 28)[:8], \n",
    "                                  reconstruction.view(ParamsConfig.BATCH_SIZE, 1, 28, 28)[:8]))\n",
    "                save_image(both.cpu(), f\"./outputs/output{epoch}.png\", nrow=num_rows)\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. RUN TRAINING FUNCTION<a name=\"training\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_dataset = AudioDataset(data_path=ParamsConfig.DATA_PATH)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = ParamsConfig.BATCH_SIZE, num_workers=0)\n",
    "\n",
    "val_dataset = AudioDataset(data_path=ParamsConfig.DATA_PATH)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=ParamsConfig.BATCH_SIZE, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 5363\n"
     ]
    }
   ],
   "source": [
    "model = VAE().to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=ParamsConfig.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (fc_mu): Linear(in_features=6272, out_features=2, bias=True)\n",
       "    (fc_logvar): Linear(in_features=6272, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=2, out_features=6272, bias=True)\n",
       "    (conv2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (conv1): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13681 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100\n",
      "Input size to Conv1 encoder: torch.Size([1, 1, 190, 167])\n",
      "Output size Conv1 encoder torch.Size([1, 64, 190, 167])\n",
      "Output size after mx-pool1 torch.Size([1, 64, 38, 167])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 2, 3, 3], expected input[1, 64, 38, 167] to have 2 channels, but got 64 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-142a16f94753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParamsConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} of {ParamsConfig.NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mval_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-ad75fbf47002>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mbce_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-32bf172bb8f9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mlatent_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_logvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_logvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx_recon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-14ce8bd5df69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output size after mx-pool1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output size Conv2 encoder'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# flatten batch of multi-channel feature maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 415\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 2, 3, 3], expected input[1, 64, 38, 167] to have 2 channels, but got 64 channels instead"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "if not os.path.exists(ParamsConfig.TRAINED_MODELS_PATH):\n",
    "    os.mkdir(ParamsConfig.TRAINED_MODELS_PATH)\n",
    "    \n",
    "for epoch in range(ParamsConfig.NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1} of {ParamsConfig.NUM_EPOCHS}\")\n",
    "    train_epoch_loss = fit(model, train_dataloader)\n",
    "    val_epoch_loss = validate(model, val_dataloader)\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_epoch_loss:.4f}\")\n",
    "    \n",
    "    #save trained model every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), \n",
    "                   os.path.join(ParamsConfig.TRAINED_MODELS_PATH, saved_model_ + str(epoch) + \"epochs.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 47, 41])\n",
      "torch.Size([1, 6016, 41])\n"
     ]
    }
   ],
   "source": [
    "shape = (1, 128, 47, 41)\n",
    "tensor_4D = torch.randn(shape)\n",
    "print(tensor_4D.shape)\n",
    "print(tensor_4D.view(1, -1, tensor_4D.size(3)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. VISUALIZE RECONSTRUCTIONS<a name=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "fig = imageio.mimsave('./images/recon_tracking1.gif', \n",
    "                      viz_helper_20d.recon_tracking_imgs[::5], duration=.05)\n",
    "\n",
    "Image(filename=\"./images/recon_tracking1.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = imageio.mimsave('./images/recon_tracking2.gif',\n",
    "                       viz_helper_20d.recon_tracking_imgs[:150:1], duration=.1)\n",
    "Image(filename=\"./images/recon_tracking2.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 2D LATENT SPACE VAE<a name=\"2d\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_helper_2d = VAEVizHelper(recon_base_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliVAE(data_size=784, encoder_szs=[400,150], latent_size=2,\n",
    "                     decoder_szs=[150,400]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, 10, 10, viz_helper_2d, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('./images/recon_tracking2d.gif', \n",
    "                viz_helper_2d.datagen_tracking_imgs[::5], duration=.05)\n",
    "Image(filename=\"./images/recon_tracking2d.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('./images/recon_tracking2d_early.gif',\n",
    "                viz_helper_2d.datagen_tracking_imgs[:200:2], duration=.1)\n",
    "Image(filename=\"./images/recon_tracking2d_early.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(viz_helper_2d.datagen_tracking_imgs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.imwrite('./images/data_final.png', \n",
    "                viz_helper_2d.datagen_tracking_imgs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References <a name=\"references\"></a>\n",
    "\n",
    "* Adam Lineberry: [GiHub](https://github.com/alineberry/alcore/blob/master/notebooks/VAE.ipynb) [Blog](http://adamlineberry.ai/vae-series/vae-code-experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    \n",
    "<button type=\"button\" class=\"btn btn-primary\" style=\"float:left; background-color:#a273f9; border:0\"><a href=\"#top\" style=\"color:white; text-decoration: none\">⇦ Back to Top</a></button>\n",
    "\n",
    "<button type=\"button\" class=\"btn btn-primary\" style=\"float:right; background-color:#BA55D3; border:0\"><a href=\"2-chord_prediction.ipynb\" style=\"color:white; text-decoration: none\">Go to Chord Detection ⇒</a></button>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
